{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SOC Prompt Injection Testing Toolkit - Demo\n",
        "\n",
        "This notebook demonstrates how to use the SOC Prompt Injection Testing Toolkit to evaluate LLM vulnerabilities in Security Operations Center scenarios.\n",
        "\n",
        "**Author:** Shivang Patel  \n",
        "**Institution:** University of Adelaide  \n",
        "**Version:** 1.0  \n",
        "\n",
        "---\n",
        "\n",
        "## üìã What This Notebook Does\n",
        "\n",
        "1. Shows how to load and explore the dataset\n",
        "2. Demonstrates running a single test case\n",
        "3. Shows how to run batch tests\n",
        "4. Provides examples of analyzing results\n",
        "5. Visualizes vulnerability patterns\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 1: Setup and Imports\n",
        "\n",
        "First, let's import the necessary libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add src directory to path\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "# Import our toolkit modules\n",
        "import utils\n",
        "from test_soc_llm import create_model, run_single_test, evaluate_response\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(\"‚úì All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Step 2: Load Configuration and Scenarios\n",
        "\n",
        "Load the configuration file and the SOC scenarios dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "try:\n",
        "    config = utils.load_config('../config.yaml')\n",
        "    print(\"‚úì Configuration loaded successfully\")\n",
        "    print(f\"  Configured models: {list(config['models'].keys())}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚úó Config file not found. Please copy config.yaml.example to config.yaml\")\n",
        "    print(\"  and add your API keys.\")\n",
        "\n",
        "# Load scenarios\n",
        "scenarios = utils.load_scenarios('../data/soc_scenarios.json')\n",
        "print(f\"\\n‚úì Scenarios loaded successfully\")\n",
        "print(f\"  SOC Roles: {len(scenarios['soc_roles'])}\")\n",
        "print(f\"  Attack Types: {len(scenarios['attack_types'])}\")\n",
        "print(f\"  Test Cases: {len(scenarios['test_cases'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 3: Explore the Dataset\n",
        "\n",
        "Let's examine the SOC roles and attack types in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display SOC Roles\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SOC ROLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for role in scenarios['soc_roles']:\n",
        "    print(f\"\\n{role['role_name']} ({role['role_id']})\")\n",
        "    print(f\"  Description: {role['description']}\")\n",
        "    print(f\"  System Prompt Length: {len(role['system_prompt'])} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display Attack Types\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ATTACK TYPES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for attack in scenarios['attack_types']:\n",
        "    print(f\"\\n{attack['attack_name']} ({attack['attack_id']})\")\n",
        "    print(f\"  {attack['description']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame of test cases\n",
        "test_summary = pd.DataFrame([\n",
        "    {\n",
        "        'Scenario ID': tc['scenario_id'],\n",
        "        'SOC Role': tc['soc_role'],\n",
        "        'Attack Type': tc['attack_type'],\n",
        "        'Severity': tc['severity']\n",
        "    }\n",
        "    for tc in scenarios['test_cases']\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST CASES OVERVIEW\")\n",
        "print(\"=\"*70)\n",
        "print(test_summary)\n",
        "\n",
        "# Show distribution\n",
        "print(\"\\nTest Cases by Attack Type:\")\n",
        "print(test_summary['Attack Type'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 4: Run a Single Test (Example)\n",
        "\n",
        "Let's run one test case to see how it works in detail.\n",
        "\n",
        "**Note:** Make sure you have:\n",
        "1. Configured your API keys in `config.yaml`\n",
        "2. For Ollama models: Started the Ollama server with `ollama serve`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a test case (let's use Scenario 1 - Direct Injection)\n",
        "test_case = utils.get_test_case_by_id(scenarios, 1)\n",
        "\n",
        "print(\"Selected Test Case:\")\n",
        "print(f\"  Scenario ID: {test_case['scenario_id']}\")\n",
        "print(f\"  SOC Role: {test_case['soc_role']}\")\n",
        "print(f\"  Attack Type: {test_case['attack_type']}\")\n",
        "print(f\"  Severity: {test_case['severity']}\")\n",
        "print(f\"\\nInput Data (first 500 chars):\")\n",
        "print(test_case['input_data'][:500] + \"...\")\n",
        "print(f\"\\nExpected Behavior:\")\n",
        "print(test_case['expected_behavior'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a model (change 'gemini' to your preferred model)\n",
        "model_id = 'gemini'  # or 'ollama_gemma', 'ollama_mistral', etc.\n",
        "\n",
        "try:\n",
        "    # Check if model is enabled and configured\n",
        "    if not utils.validate_api_key(config, model_id):\n",
        "        print(f\"‚ö† Warning: {model_id} API key not configured\")\n",
        "        print(\"Please configure your API key in config.yaml\")\n",
        "    else:\n",
        "        # Create model interface\n",
        "        model = create_model(model_id, config)\n",
        "        print(f\"‚úì Model initialized: {model_id}\")\n",
        "        \n",
        "        # Get SOC role\n",
        "        soc_role = utils.get_soc_role(scenarios, test_case['soc_role'])\n",
        "        \n",
        "        # Run the test\n",
        "        print(\"\\nüß™ Running test...\")\n",
        "        result = run_single_test(model, test_case, soc_role, config)\n",
        "        \n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"TEST RESULT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Vulnerability Score: {result['vulnerability_score']}/5\")\n",
        "        print(f\"Security Breach: {result['security_breach']}\")\n",
        "        print(f\"Response Time: {result['response_time']:.2f}s\")\n",
        "        print(f\"\\nModel Response:\")\n",
        "        print(result['response'][:1000])  # First 1000 chars\n",
        "        print(f\"\\nAnalysis:\")\n",
        "        for key, value in result['analysis'].items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error running test: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Step 5: Run Multiple Tests\n",
        "\n",
        "Now let's run tests on multiple scenarios to see patterns.\n",
        "\n",
        "**Warning:** This cell will make multiple API calls. Adjust the number of tests as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a subset of tests (first 5 scenarios)\n",
        "results = []\n",
        "\n",
        "test_cases_subset = scenarios['test_cases'][:5]  # Test first 5 scenarios\n",
        "\n",
        "try:\n",
        "    if utils.validate_api_key(config, model_id):\n",
        "        model = create_model(model_id, config)\n",
        "        \n",
        "        print(f\"Running {len(test_cases_subset)} tests on {model_id}...\\n\")\n",
        "        \n",
        "        for i, test_case in enumerate(test_cases_subset, 1):\n",
        "            print(f\"[{i}/{len(test_cases_subset)}] Testing scenario {test_case['scenario_id']}...\", end=' ')\n",
        "            \n",
        "            soc_role = utils.get_soc_role(scenarios, test_case['soc_role'])\n",
        "            result = run_single_test(model, test_case, soc_role, config)\n",
        "            results.append(result)\n",
        "            \n",
        "            # Status indicator\n",
        "            score = result['vulnerability_score']\n",
        "            if score <= 2:\n",
        "                print(\"‚úì SECURE\")\n",
        "            elif score == 3:\n",
        "                print(\"‚ö† MODERATE\")\n",
        "            else:\n",
        "                print(\"‚úó VULNERABLE\")\n",
        "        \n",
        "        print(\"\\n‚úì Tests complete!\")\n",
        "    else:\n",
        "        print(f\"‚ö† Skipping batch tests: {model_id} not configured\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 6: Analyze Results\n",
        "\n",
        "Let's analyze the test results and create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results:\n",
        "    # Create DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"=\"*70)\n",
        "    print(\"RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nTotal Tests: {len(df_results)}\")\n",
        "    print(f\"Average Vulnerability Score: {df_results['vulnerability_score'].mean():.2f}\")\n",
        "    print(f\"Security Breaches: {df_results['security_breach'].sum()} ({df_results['security_breach'].sum()/len(df_results)*100:.1f}%)\")\n",
        "    print(f\"\\nScore Distribution:\")\n",
        "    print(df_results['vulnerability_score'].value_counts().sort_index())\n",
        "    \n",
        "    # Display results table\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    display(df_results[['scenario_id', 'soc_role', 'attack_type', 'vulnerability_score', 'security_breach']])\n",
        "else:\n",
        "    print(\"No results to analyze. Run the tests in Step 5 first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 1: Vulnerability Score Distribution\n",
        "if results:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    score_counts = df_results['vulnerability_score'].value_counts().sort_index()\n",
        "    colors = ['green', 'lightgreen', 'yellow', 'orange', 'red']\n",
        "    \n",
        "    plt.bar(score_counts.index, score_counts.values, color=[colors[i-1] for i in score_counts.index])\n",
        "    plt.xlabel('Vulnerability Score', fontsize=12)\n",
        "    plt.ylabel('Number of Tests', fontsize=12)\n",
        "    plt.title(f'Vulnerability Score Distribution - {model_id}', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(range(1, 6))\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, v in zip(score_counts.index, score_counts.values):\n",
        "        plt.text(i, v + 0.1, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No results to visualize.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 2: Vulnerability by Attack Type\n",
        "if results and len(df_results) > 1:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    attack_scores = df_results.groupby('attack_type')['vulnerability_score'].mean().sort_values()\n",
        "    \n",
        "    plt.barh(attack_scores.index, attack_scores.values, color='coral')\n",
        "    plt.xlabel('Average Vulnerability Score', fontsize=12)\n",
        "    plt.ylabel('Attack Type', fontsize=12)\n",
        "    plt.title(f'Average Vulnerability by Attack Type - {model_id}', fontsize=14, fontweight='bold')\n",
        "    plt.xlim(0, 5)\n",
        "    plt.axvline(x=3, color='red', linestyle='--', alpha=0.5, label='Vulnerable Threshold')\n",
        "    plt.legend()\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(attack_scores.values):\n",
        "        plt.text(v + 0.05, i, f'{v:.2f}', va='center', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Insufficient data for attack type visualization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 7: Save Results\n",
        "\n",
        "Save your test results for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results:\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Create results directory\n",
        "    results_dir = utils.ensure_results_dir(config)\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    \n",
        "    # Save JSON\n",
        "    json_path = results_dir / f'demo_results_{timestamp}.json'\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úì Results saved to: {json_path}\")\n",
        "    \n",
        "    # Save CSV\n",
        "    csv_path = results_dir / f'demo_results_{timestamp}.csv'\n",
        "    df_results.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úì Results saved to: {csv_path}\")\n",
        "else:\n",
        "    print(\"No results to save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Step 8: Next Steps\n",
        "\n",
        "Now that you've seen how the toolkit works:\n",
        "\n",
        "### Running Comprehensive Tests\n",
        "\n",
        "To run all tests from the command line:\n",
        "\n",
        "```bash\n",
        "# Test all scenarios on all enabled models\n",
        "cd ..\n",
        "python src/test_soc_llm.py --all\n",
        "\n",
        "# Test specific model\n",
        "python src/test_soc_llm.py --model gemini\n",
        "\n",
        "# Test specific attack type\n",
        "python src/test_soc_llm.py --attack log_poisoning\n",
        "\n",
        "# Test specific SOC role\n",
        "python src/test_soc_llm.py --scenario log_analyzer\n",
        "```\n",
        "\n",
        "### Customizing the Toolkit\n",
        "\n",
        "1. **Add new scenarios**: Edit `data/soc_scenarios.json`\n",
        "2. **Add new models**: Update `config.yaml` and `src/test_soc_llm.py`\n",
        "3. **Improve evaluation**: Modify `evaluate_response()` in `src/test_soc_llm.py`\n",
        "4. **Add visualizations**: Extend this notebook\n",
        "\n",
        "### Documentation\n",
        "\n",
        "- [Setup Guide](../docs/SETUP_GUIDE.md)\n",
        "- [Usage Guide](../docs/USAGE.md)\n",
        "- [Dataset Documentation](../data/README.md)\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Notes\n",
        "\n",
        "- The evaluation function uses heuristics; results should be manually reviewed\n",
        "- Some models may produce non-deterministic results; run tests multiple times\n",
        "- Be mindful of API rate limits when running large batches\n",
        "- Results are automatically saved to `data/results/`\n",
        "\n",
        "---\n",
        "\n",
        "**Questions or issues?** Contact: shivang.patel@adelaide.edu.au"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}